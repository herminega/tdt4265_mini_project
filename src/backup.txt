def get_dataset(data_dir, subset="train", augment=False):
    """Loads MRI dataset from the given subset (train/test) directory."""
    
    subset_dir = os.path.join(data_dir, subset)  # e.g., "/cluster/.../HNTS-MRG/train"
    
    transforms = [
        LoadImage(image_only=True),  
        EnsureChannelFirst(),  
        ScaleIntensity(),  
    ]

    if augment:
        transforms.extend([
            RandFlip(prob=0.5, spatial_axis=0),
            RandRotate(range_x=0.1, prob=0.5),
            RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5)
        ])

    # Recursively find all .nii.gz files inside the subset directory
    images = sorted(glob.glob(os.path.join(subset_dir, "**", "*"), recursive=True))
    dataset = Dataset(data=images, transform=Compose(transforms))

    return dataset






def get_mri_data_loader(data_dir, batch_size=2, validation_fraction=0.1):
    """Loads MRI dataset using MONAI and returns train & validation loaders."""

    transforms = Compose([
        LoadImage(image_only=True),  # Load MRI images
        EnsureChannelFirst(),        # Ensure shape (C, H, W, D)
        ScaleIntensity(),            # Normalize intensity
        ToTensor()                   # Convert to PyTorch tensor
    ])
    
    # Load all images & segmentation masks
    images = sorted([os.path.join(data_dir, "images", f) for f in os.listdir(os.path.join(data_dir, "images")) if f.endswith(".nii.gz")])
    masks = sorted([os.path.join(data_dir, "masks", f) for f in os.listdir(os.path.join(data_dir, "masks")) if f.endswith(".nii.gz")])

    # Ensure image-mask pairs match
    assert len(images) == len(masks), "Mismatch between number of images and masks!"

    dataset = Dataset(data=[{"image": img, "label": mask} for img, mask in zip(images, masks)], transform=transforms)

    # Split dataset into train/validation
    num_samples = len(dataset)
    indices = torch.randperm(num_samples)
    split_idx = int(num_samples * validation_fraction)
    
    train_indices, val_indices = indices[split_idx:], indices[:split_idx]
    
    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(train_indices))
    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(val_indices))

    return train_loader, val_loader



import os
import torch
from monai.transforms import (
    LoadImaged,  
    EnsureChannelFirstd,
    ScaleIntensityd,
    ResizeWithPadOrCropd,
    ToTensord,
    Compose,
)
from monai.data import Dataset, DataLoader, pad_list_data_collate
import nibabel as nib


def get_mri_data_loader(data_dir: str, subset="train", batch_size=2, validation_fraction=0.1):
    """
    Loads MRI dataset using MONAI, ensuring correct pairing of images & masks.
    - subset: "train" or "test"
    - batch_size: Number of samples per batch
    """

    transforms = Compose([
        LoadImaged(keys=["image", "label"]),
        EnsureChannelFirstd(keys=["image", "label"]),
        ScaleIntensityd(keys=["image"]),
        ResizeWithPadOrCropd(keys=["image", "label"], spatial_size=(128, 128, 64)),  # Ensure same size
        ToTensord(keys=["image", "label"]),
    ])

    # Path to train or test directory
    subset_dir = os.path.join(data_dir, subset)
    patient_folders = sorted(os.listdir(subset_dir))

    # Store paired image-mask file paths
    data_list = []
    
    for patient_id in patient_folders:
        preRT_path = os.path.join(subset_dir, patient_id, "preRT")

        # Find image & mask
        image_file = next((f for f in os.listdir(preRT_path) if f.endswith("T2.nii.gz")), None)
        mask_file = next((f for f in os.listdir(preRT_path) if f.endswith("mask.nii.gz")), None)

        full_image_path = os.path.join(preRT_path, image_file)
        full_mask_path = os.path.join(preRT_path, mask_file)
        
        image_shape = nib.load(full_image_path).shape
        mask_shape = nib.load(full_mask_path).shape


        data_list.append({
            "image": full_image_path,
            "label": full_mask_path
        })

    #Use dictionary-based Dataset
    dataset = Dataset(data=data_list, transform=transforms)

    # Split into training & validation sets
    num_samples = len(dataset)
    indices = torch.randperm(num_samples)
    split_idx = int(num_samples * validation_fraction)

    train_indices, val_indices = indices[split_idx:], indices[:split_idx]

    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(train_indices), collate_fn=pad_list_data_collate, num_workers=0)
    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(val_indices), collate_fn=pad_list_data_collate, num_workers=0)

    return train_loader, val_loader


import torch
import torch.optim as optim
from monai.losses import DiceLoss
from model import UNet3D
from dataloader import get_debug_dataloader  # ✅ Use simpler dataloader
import psutil

class Trainer:
    """Trains the UNet3D model on the MRI dataset."""
    torch.cuda.empty_cache()  # ✅ Clears cached memory
    torch.backends.cudnn.benchmark = False  # ✅ Optimizes GPU execution
    
    def __init__(self, data_dir, in_channels, out_channels, batch_size=2, validation_fraction=0.1, subset="train"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")  
        
        self.model = UNet3D(in_channels=in_channels, out_channels=out_channels).to(self.device)

        self.train_loader, self.val_loader = get_debug_dataloader(data_dir, batch_size=batch_size)
        self.optimizer = optim.Adam(self.model.parameters(), 1e-4)
        self.loss_function = DiceLoss(sigmoid=True).to(self.device)

    def train(self, epochs):
        for epoch in range(epochs):
            print(f"Epoch {epoch+1} training started")
            self.model.train()
            epoch_loss = 0
            
            for batch_idx, batch in enumerate(self.train_loader):
                inputs = batch["image"].to(self.device, dtype=torch.float32)
                labels = batch["label"].to(self.device, dtype=torch.float32)

                self.optimizer.zero_grad()
                outputs = self.model(inputs)  # Forward pass
                loss = self.loss_function(outputs, labels)  # ⚠ Remove extra sigmoid!
                
                # Detect NaN loss and stop training early
                if torch.isnan(loss):
                    print(f"NaN detected in loss at batch {batch_idx}, stopping training.")
                    return  

                loss.backward()
                self.optimizer.step()
                
                epoch_loss += loss.item()

                if batch_idx % 10 == 0:  # Print every 10 batches
                    print(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")
            
            print(f"Epoch {epoch+1} completed, Avg Loss: {epoch_loss / len(self.train_loader):.4f}")


    def validate(self):
        self.model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in self.val_loader:
                inputs = batch.to(self.device)
                outputs = self.model(inputs)
                loss = self.loss_function(outputs, inputs)
                val_loss += loss.item()
        print(f"Validation Loss: {val_loss/len(self.val_loader)}")
    
    def save_model(self, path="results/model.pth"):
        torch.save(self.model.state_dict(), path)
        print(f"Model saved to {path}")
    
if __name__ == "__main__":
    # Set your dataset directory
    data_dir = "/cluster/projects/vc/data/mic/open/HNTS-MRG"
    

    # Initialize the Trainer
    trainer = Trainer(data_dir=data_dir, in_channels=1, out_channels=1, batch_size=1, validation_fraction=0.1, subset="train")

    # Run training for 4 epochs
    trainer.train(epochs=4)




        ### Loss functions options ###
        class_weights = torch.tensor([0.1, 1.5, 1.0]).to(self.device)
        # 1. (Generalized Dice Loss with softmax)
        #self.loss_criterion = GeneralizedDiceLoss(softmax=True, include_background=True) 
        # 2. Class weights for handling imbalance
        #self.loss_criterion = DiceLoss(
        #    softmax=True, 
        #    weight=class_weights)
        # 3. Combined Dice + Cross-Entropy Loss
        self.loss_criterion = DiceCELoss(to_onehot_y=False, softmax=True, lambda_dice=0.7, lambda_ce=0.3, weight=class_weights)
        # 4. Focal loss [did not work well]
        #self.loss_criterion = FocalLoss(
        #    include_background=True,
        #    to_onehot_y=False,  # already using onehot labels in transform
        #    gamma=2.0,
        #    weight=class_weights,  # optional
        #    reduction = "mean"
        #)
        # 5. Combined Dice + Focal Loss
        #self.loss_criterion = DiceFocalLoss(
        #    to_onehot_y=False,
        #    softmax=True,
        #    gamma=1.0, 
        #    lambda_dice = 0.8,
        #    lambda_focal = 0.2,
            #weight=class_weights,  # optional
        #    reduction = "mean"
        #)
        

        # Option 1: Just CE with smoothing
        #self.loss_criterion = LabelSmoothingCrossEntropyLoss(smoothing=0.1)

        # Option 2: Combine it with Dice
        #dice = DiceLoss(softmax=True)
        #ce = LabelSmoothingCrossEntropyLoss(smoothing=0.1)

        #def combined_loss(pred, target):
        #    return 0.6 * dice(pred, target) + 0.4 * ce(pred, target)

        #self.loss_criterion = combined_loss











class Trainer:
    def __init__(self, data_dir, batch_size, learning_rate, early_stop_count, epochs, in_channels=1, out_channels=3):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        self.model = get_model("nnunet", in_channels, out_channels).to(self.device)
        self.train_loader, self.val_loader = get_mri_dataloader(data_dir, "train", batch_size, validation_fraction=0.1)

        class_weights = torch.tensor([0.5, 1.0, 1.2]).to(self.device)
        self.loss_criterion = DiceCELoss(to_onehot_y=True, softmax=True, smooth_dr=0.0001, lambda_dice=0.75, lambda_ce=0.35)

        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=8, verbose=True, min_lr=1e-6)
        # Alternative schedulers can be used:
        # torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=10, T_mult=2)
        # torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=epochs)

        self.scaler = GradScaler()
        self.epochs = epochs
        self.early_stop_count = early_stop_count
        self.global_step = 0

        self.train_history = dict(loss=collections.OrderedDict(), dice=collections.OrderedDict())
        self.validation_history = dict(loss=collections.OrderedDict(), dice=collections.OrderedDict())

        self.checkpoint_dir = pathlib.Path("checkpoints")
        self.dice_metric = DiceMetric(include_background=False, reduction="none", get_not_nans=True)
        
        # A counter to only print gradient stats every N steps.
        self.grad_monitor_interval = 50

    def monitor_gradients(self):
        """ Log gradient norms for each parameter. """
        grad_norms = []
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                norm = param.grad.data.norm(2).item()
                grad_norms.append((name, norm))
        print("Gradient norms:")
        for name, norm in grad_norms:
            print(f"  {name}: {norm:.4f}")

    def monitor_output_range(self, outputs):
        """ Log some basic statistics about the model outputs. """
        print(f"Output stats: min={outputs.min().item():.4f}, max={outputs.max().item():.4f}, mean={outputs.mean().item():.4f}")

    def train_step(self, inputs, labels):
        self.model.train()
        self.optimizer.zero_grad()

        with autocast(device_type=self.device.type):
            outputs = self.model(inputs)
            loss = self.loss_criterion(outputs, labels)
        
        # Monitor model output range; helps in diagnosing activation saturation.
        self.monitor_output_range(outputs.detach())

        # Scale loss and perform backward pass.
        self.scaler.scale(loss).backward()

        # Optionally clip gradients to avoid explosion.
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)

        # Monitor gradients every few steps.
        if self.global_step % self.grad_monitor_interval == 0:
            self.monitor_gradients()

        self.scaler.step(self.optimizer)
        self.scaler.update()

        return loss.item(), outputs, labels

    def validate(self, save=True, output_dir="results/predictions"):
        self.model.eval()
        self.dice_metric.reset()
        output_dir = pathlib.Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        total_loss, num_total, num_skipped = 0, 0, 0
        total_TP, total_FP, total_FN = {1: 0, 2: 0}, {1: 0, 2: 0}, {1: 0, 2: 0}

        with torch.no_grad():
            for idx, batch in enumerate(self.val_loader):
                inputs, labels = batch["image"].to(self.device), batch["label"].to(self.device)
                
                with autocast(device_type=self.device.type):
                    outputs = self.model(inputs)
                    loss = self.loss_criterion(outputs, labels)
                
                total_loss += loss.item()

                # Detach outputs to avoid retaining computation graph.
                softmax_outputs = torch.softmax(outputs.detach(), dim=1)
                pred_indices = torch.argmax(softmax_outputs, dim=1)
                onehot_pred = one_hot(pred_indices.unsqueeze(1), num_classes=3)  # already detached
                onehot_labels = one_hot(labels.detach().long(), num_classes=3)
                self.dice_metric(onehot_pred, onehot_labels)
                
                # Also compute TP/FP/FN for precision/recall.
                true = labels.squeeze(1)  # [B, H, W, D]
                if true.sum() == 0:
                    print(f"[Skip] Sample {idx} has no tumor in ground truth.")
                    num_skipped += 1
                    continue
                num_total += 1

                for cls in [1, 2]:
                    TP = ((pred_indices == cls) & (true == cls)).sum().item()
                    FP = ((pred_indices == cls) & (true != cls)).sum().item()
                    FN = ((pred_indices != cls) & (true == cls)).sum().item()
                    total_TP[cls] += TP
                    total_FP[cls] += FP
                    total_FN[cls] += FN

                if save and idx < 5:
                    save_predictions(inputs[0], labels[0], softmax_outputs[0], idx, output_dir)

        print(f"[Summary] Skipped {num_skipped}/{num_total + num_skipped} validation batches (no tumor present)")
        avg_loss = total_loss / len(self.val_loader)
        per_class_dice, _ = self.dice_metric.aggregate()
        per_class_dice = per_class_dice.cpu().numpy()
        class_dice_means = np.nanmean(per_class_dice, axis=0)

        class1_dice = class_dice_means[0]
        class2_dice = class_dice_means[1]
        mean_dice = np.nanmean(class_dice_means)

        self.validation_history["loss"][self.global_step] = avg_loss
        self.validation_history["dice"][self.global_step] = mean_dice
        self.validation_history.setdefault("dice_class1", {})[self.global_step] = class1_dice
        self.validation_history.setdefault("dice_class2", {})[self.global_step] = class2_dice

        precision_class1 = total_TP[1] / (total_TP[1] + total_FP[1] + 1e-6)
        recall_class1 = total_TP[1] / (total_TP[1] + total_FN[1] + 1e-6)
        precision_class2 = total_TP[2] / (total_TP[2] + total_FP[2] + 1e-6)
        recall_class2 = total_TP[2] / (total_TP[2] + total_FN[2] + 1e-6)

        return {
            "loss": avg_loss,
            "mean_dice": mean_dice,
            "dice_class1": class1_dice,
            "dice_class2": class2_dice,
            "precision_class1": precision_class1,
            "recall_class1": recall_class1,
            "precision_class2": precision_class2,
            "recall_class2": recall_class2,
        }

    def train(self):
        for epoch in range(1, self.epochs + 1):
            print(f"\n=== Epoch {epoch} ===")
            # Optionally freeze encoder weights for the first few epochs.
            # freeze_encoder(self.model, freeze=(epoch <= 5))

            epoch_loss = 0.0
            self.dice_metric.reset()
            progress_bar = tqdm(self.train_loader, desc="Training", dynamic_ncols=True)
            total_TP, total_FP, total_FN = {1: 0, 2: 0}, {1: 0, 2: 0}, {1: 0, 2: 0}

            for batch in progress_bar:
                inputs, labels = batch["image"].to(self.device), batch["label"].to(self.device)
                loss, outputs, labels = self.train_step(inputs, labels)
                
                if self.global_step == 0:
                    print(f"labels shape: {labels.shape}")
                    print(f"outputs shape: {outputs.shape}")

                self.global_step += 1
                epoch_loss += loss
                
                # For metric computation, ensure no extra gradient history is retained.
                softmax_outputs = torch.softmax(outputs.detach(), dim=1)
                pred_indices = torch.argmax(softmax_outputs, dim=1)
                onehot_pred = one_hot(pred_indices.unsqueeze(1), num_classes=3)
                onehot_labels = one_hot(labels.detach().long(), num_classes=3)
                self.dice_metric(onehot_pred, onehot_labels)
                
                self.train_history["loss"][self.global_step] = loss
                progress_bar.set_postfix(loss=f"{loss:.4f}")

                true = labels.squeeze(1)
                for cls in [1, 2]:
                    TP = ((pred_indices == cls) & (true == cls)).sum().item()
                    FP = ((pred_indices == cls) & (true != cls)).sum().item()
                    FN = ((pred_indices != cls) & (true == cls)).sum().item()
                    total_TP[cls] += TP
                    total_FP[cls] += FP
                    total_FN[cls] += FN

            per_class_dice, _ = self.dice_metric.aggregate()
            per_class_dice = per_class_dice.cpu().numpy()
            class_dice_means = np.nanmean(per_class_dice, axis=0)
            class1_dice = class_dice_means[0]
            class2_dice = class_dice_means[1]
            mean_dice = np.nanmean(class_dice_means)
            avg_loss = epoch_loss / len(self.train_loader)

            precision_class1 = total_TP[1] / (total_TP[1] + total_FP[1] + 1e-6)
            recall_class1 = total_TP[1] / (total_TP[1] + total_FN[1] + 1e-6)
            precision_class2 = total_TP[2] / (total_TP[2] + total_FP[2] + 1e-6)
            recall_class2 = total_TP[2] / (total_TP[2] + total_FN[2] + 1e-6)

            self.train_history["dice"][self.global_step] = mean_dice
            self.train_history.setdefault("dice_class1", {})[self.global_step] = class1_dice
            self.train_history.setdefault("dice_class2", {})[self.global_step] = class2_dice

            train_metrics = {
                "loss": avg_loss,
                "mean_dice": mean_dice,
                "dice_class1": class1_dice,
                "dice_class2": class2_dice,
                "precision_class1": precision_class1,
                "recall_class1": recall_class1,
                "precision_class2": precision_class2,
                "recall_class2": recall_class2,
            }

            val_metrics = self.validate()
            self.scheduler.step(val_metrics["loss"])  # Adjust learning rate based on validation loss.

            # Optional: print current learning rate.
            for param_group in self.optimizer.param_groups:
                print(f"Current LR: {param_group['lr']:.6f}")
            
            save_checkpoint(self)
            log_metrics(epoch, train_metrics, val_metrics)
            
            if should_early_stop(self):
                print("Early stopping triggered.")
                break